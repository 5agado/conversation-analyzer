{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-27T11:08:33.075061",
     "start_time": "2017-04-27T11:08:32.597034"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "sns.set_context(\"paper\", font_scale=1.2)\n",
    "\n",
    "%matplotlib notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.getcwd(), \"..\\..\\src\"))\n",
    "\n",
    "import util.io as mio\n",
    "from util import statsUtil\n",
    "import util.plotting as mplot\n",
    "from model.conversationDataframe import ConversationDataframe\n",
    "from stats.iConvStats import IConvStats\n",
    "from stats.wordsCountStats import WordsCountStats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "This notebook is used as utility/tool for analysis of text.\n",
    "The goal is to get some insight about the structure, content and quality of the text.\n",
    "\n",
    "Examples: analysis of CV, personal articles, job ads. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Text\n",
    "Load text you want to analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-27T10:24:42.203584",
     "start_time": "2017-04-27T10:24:41.736557"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_text(filepaths):\n",
    "    \"\"\"\n",
    "    Load text you want to analyse.\n",
    "    :param filepaths: list of paths to text files to load\n",
    "    :return: single string representing all retrieved text\n",
    "    \"\"\"\n",
    "    text = \"\"\n",
    "    for path in filepaths:\n",
    "        with open(path, 'r', encoding='UTF-8') as f:\n",
    "            text += \"\\n\"+f.read()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-27T11:08:36.282245",
     "start_time": "2017-04-27T11:08:35.816218"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = load_text([\"\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Stats\n",
    "Length, count and richness, Ngram distribution and mosr relevant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-27T11:00:49.533548",
     "start_time": "2017-04-27T11:00:49.054521"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words = statsUtil.getWords(text)\n",
    "types = set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-27T11:00:50.806621",
     "start_time": "2017-04-27T11:00:50.330594"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Total length: {:.0f}\".format(len(text)))\n",
    "print(\"Tokens count: {:.0f}\".format(len(words)))\n",
    "print(\"Distinct tokens count: {:.0f}\".format(len(set(words))))\n",
    "print(\"Lexical richness: {0:.5f}\".format(len(types)/len(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-27T10:39:33.409558",
     "start_time": "2017-04-27T10:39:32.897529"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_most_common(most_common_ngrams, n_most, join=False):\n",
    "    most_common_ngrams, count = zip(*most_common_ngrams.most_common(n_most))\n",
    "    if join:\n",
    "        most_common_ngrams = [\" \".join(list(e)) for e in most_common_ngrams]\n",
    "    ax = sns.pointplot(y=most_common_ngrams, x=count)\n",
    "    sns.plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-27T11:00:59.309107",
     "start_time": "2017-04-27T11:00:58.750075"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Most common words\n",
    "words_count = Counter(words)\n",
    "\n",
    "# Plot most common words\n",
    "plot_most_common(words_count, n_most=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-27T11:22:26.021703",
     "start_time": "2017-04-27T11:22:25.406668"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "most_common_bigrams = Counter(nltk.bigrams(words))\n",
    "\n",
    "plot_most_common(most_common_bigrams, 20, join=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-27T11:02:18.924661",
     "start_time": "2017-04-27T11:02:18.369629"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "most_common_trigrams = Counter(nltk.trigrams(words))\n",
    "\n",
    "plot_most_common(most_common_trigrams, 20, join=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-27T10:54:36.610218",
     "start_time": "2017-04-27T10:54:36.161192"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get most relevant words using TF-IDF\n",
    "# For this statistic we need additional pieces of text to compare with our speech transcript\n",
    "# we can simply load some corpora from NLTK \n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def get_top_features(text, n):\n",
    "    # Load corpora for different genres\n",
    "    c1 = nltk.corpus.gutenberg.raw('carroll-alice.txt')\n",
    "    c2 = nltk.corpus.inaugural.raw(\"2009-Obama.txt\")\n",
    "    c3 = nltk.corpus.webtext.raw(\"firefox.txt\")\n",
    "    # Load english stopwords\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "\n",
    "    # Compute TF-IDF matrix and print top results for our speech\n",
    "    vectorizer = TfidfVectorizer(analyzer='word',stop_words=stops, ngram_range=(2,3))\n",
    "    tfIdf = vectorizer.fit_transform([text, c1, c2, c3]).toarray()\n",
    "    indices = np.argsort(tfIdf[0])[::-1]\n",
    "    features = vectorizer.get_feature_names()\n",
    "    top_features = [features[i] for i in indices[:n] if tfIdf[0][i]!=0]\n",
    "    return top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-27T11:02:52.289569",
     "start_time": "2017-04-27T11:02:50.892489"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_top_features(text, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prose Stats\n",
    "“Over the whole document, make the average sentence length 15-20 words, 25-33 syllables and 75-100 characters.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-27T11:17:37.943226",
     "start_time": "2017-04-27T11:17:37.501200"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# prose stats\n",
    "sentences = list(filter(lambda x : len(x)>0, map(str.strip, re.split(r'[\\.\\?!\\n]', text))))\n",
    "sen_len = [len(sent) for sent in sentences]\n",
    "print(\"Average sentence len {}. Max {}, min {}\".format(np.mean(sen_len), max(sen_len), min(sen_len)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-27T11:18:08.590979",
     "start_time": "2017-04-27T11:18:08.143953"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for sent in sentences:\n",
    "    if len(sent)>300:\n",
    "        print(\"* \" + sent)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:kaggle]",
   "language": "python",
   "name": "conda-env-kaggle-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "67px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
